{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables we need are :\n",
    "1. URL of the News Article\n",
    "2. Domain Address of the News Source\n",
    "3. News Source Name\n",
    "4. Status(of the query)\n",
    "5. Title\n",
    "6. Author Name\n",
    "7. Publication Date\n",
    "8. Description\n",
    "9. URL of the most relevent image from the news article\n",
    "10. Full text of Article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.techradar.com/in/news/video-conferencing-calls-may-finally-be-about-to-get-a-bit-less-boring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.techradar.com\n"
     ]
    }
   ],
   "source": [
    "source_url = url.split('/')[2]\n",
    "print(source_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "techradar\n"
     ]
    }
   ],
   "source": [
    "source_name = source_url.split('.')[1]\n",
    "print(source_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = req.get(url)\n",
    "status = r.status_code\n",
    "article = r.text\n",
    "soup = BeautifulSoup(article,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video conferencing calls may finally be about to get a bit less boring\n"
     ]
    }
   ],
   "source": [
    "title = soup.h1.get_text()\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthony Spadafora\n"
     ]
    }
   ],
   "source": [
    "author = soup.find('span', {'style':'white-space: nowrap'}).get_text()\n",
    "print(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-07T23:53:07+00:00\n"
     ]
    }
   ],
   "source": [
    "publishedAt = soup.find('meta',{'name':'pub_date'}).get('content')\n",
    "print(publishedAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cisco plans to add improved audience interaction to Webex with new dynamic polls, Q&As, quizzes, word clouds, surveys and more.\n"
     ]
    }
   ],
   "source": [
    "description = soup.find('meta',{'name':'description'}).get('content')\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cdn.mos.cms.futurecdn.net/aifiPTc3XXos48vUogJgT5-320-80.jpg\n"
     ]
    }
   ],
   "source": [
    "urlToImage = soup.find('img',{'alt':'Slido'})['src']\n",
    "print(urlToImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content():\n",
    "  parse4content =  soup.find('div', {'id':'article-body'})\n",
    "  ad = parse4content.find_all('p')\n",
    "  a = \"\"\n",
    "  for _ in ad:\n",
    "    a += _.get_text().strip()\n",
    "  \n",
    "  return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Keeping users engaged and part of the conversation while using video conferencing software has proven to be difficult with employees working from home which is why Cisco has announced its plans to acquire Slido.Slido is a technology platform that provides a best-in-class audience interaction platform that enables higher levels of users engagement before, during and after meetings and events.Once the acquisition is complete, Slido's technology will become part of the Cisco Webex platform and enhance the company's ability to offer new levels of inclusive audience engagement across both in-person and virtual experiences.In a blog post announcing the acquisition, VP and GM of Webex Meetings at Cisco's Team Collaboration Group Abhay Kulkarni explained that the company's goal is to deliver Webex experiences that are ten times better than in-person interactions. While a lofty goal, the company believes that Slido's technology will help it make these experiences inclusive and equal for all.While remote meetings and events have enabled people all over the world to connect during the pandemic, 98 percent of respondents from a recent global workforce survey say that they expect future meetings to include remote participants. This is one of the reason Cisco and other video conferencing software makers are continuing to add new capabilities to their services despite the fact that many companies plan on returning to the office soon.Slido, which has over 7m monthly participants, provides users with an inclusive audience engagement platform that enables real-time feedback and insight before, during and after any meeting or event using dynamic polls, Q&As, quizzes, word clouds, surveys and more.Using the service, audience interaction can be set up in less than a minute and participants can join the audience view with a simple event code or link on their mobile devices or computers. Once a participant joins, they can respond to polls, ask questions or provide feedback on a meeting.Cisco plans to add Slido's capabilities to Webex once the acquisition is complete and its technology will also align with the company's strict data policies so that organizations' internal polls, quizzes and feedback are both secure and private.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiling bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 14 12:55:57 2020    restats\n",
      "\n",
      "         57272 function calls in 0.059 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 117 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.059    0.059 {built-in method builtins.exec}\n",
      "        1    0.001    0.001    0.058    0.058 <string>:2(<module>)\n",
      "        1    0.000    0.000    0.058    0.058 <string>:4(main)\n",
      "        1    0.000    0.000    0.055    0.055 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\site-packages\\bs4\\__init__.py:115(__init__)\n",
      "        1    0.000    0.000    0.055    0.055 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\site-packages\\bs4\\__init__.py:427(_feed)\n",
      "        1    0.000    0.000    0.055    0.055 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:369(feed)\n",
      "        1    0.000    0.000    0.055    0.055 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\html\\parser.py:104(feed)\n",
      "        2    0.004    0.002    0.055    0.027 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\html\\parser.py:134(goahead)\n",
      "      521    0.006    0.000    0.033    0.000 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\html\\parser.py:301(parse_starttag)\n",
      "      521    0.002    0.000    0.019    0.000 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:119(handle_starttag)\n",
      "      521    0.002    0.000    0.016    0.000 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\site-packages\\bs4\\__init__.py:678(handle_starttag)\n",
      "      425    0.001    0.000    0.010    0.000 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\html\\parser.py:386(parse_endtag)\n",
      "     1073    0.002    0.000    0.010    0.000 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\site-packages\\bs4\\__init__.py:541(endData)\n",
      "      543    0.001    0.000    0.008    0.000 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:171(handle_endtag)\n",
      "      521    0.001    0.000    0.007    0.000 C:\\Users\\Dabu bahya\\miniconda3\\envs\\tf\\lib\\site-packages\\bs4\\__init__.py:716(handle_endtag)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x5f68e88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "\n",
    "cProfile.run('''\n",
    "result = []\n",
    "main_dict = {}\n",
    "def main():\n",
    "  soup = BeautifulSoup(article,'html.parser')\n",
    "  main_dict[\"source_url\"] = source_url\n",
    "  main_dict[\"source_name\"] = source_name\n",
    "  main_dict[\"title\"] = title\n",
    "  main_dict[\"author\"] = author\n",
    "  main_dict[\"publishedAt\"] = publishedAt\n",
    "  main_dict[\"description\"] = description\n",
    "  main_dict[\"urlToImage\"] = urlToImage\n",
    "  main_dict[\"article\"] = content()\n",
    "  result.append(main_dict)\n",
    "  return result\n",
    "   \n",
    "main()''','restats')\n",
    "\n",
    "import pstats\n",
    "p = pstats.Stats('restats')\n",
    "p.sort_stats('cumtime').print_stats(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Xpath to scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video conferencing calls may finally be about to get a bit less boring\n"
     ]
    }
   ],
   "source": [
    "from lxml.html import fromstring, tostring\n",
    "tree = fromstring(article)\n",
    "title = tree.xpath('//*[@id=\"main\"]/article/header/h1')\n",
    "title = title[0].text\n",
    "print(title)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[<Element link at 0x50ebf98>]\n"
     ]
    }
   ],
   "source": [
    "srcurl = tree.xpath('/html/head/link[28]')\n",
    "print(srcurl[0].text)\n",
    "print(srcurl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
